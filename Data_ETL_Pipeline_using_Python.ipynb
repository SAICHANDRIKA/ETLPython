{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **1. EXTRACT**"
      ],
      "metadata": {
        "id": "5INE4CJIfn5S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ETL stands for extract, transform and load. For developing ETL pipelines, we need to extract data from various sources, transform it into a usable format, and load it into a data warehouse or database for analysis.\n",
        "\n",
        "Task is to develop a data ETL pipeline for the Fashion MNIST dataset. The dataset contains 70,000 grayscale images of size 28×28 pixels, categorized into ten clothing categories like t-shirts, dresses, sneakers, etc. The pipeline should extract the dataset from the data source, and perform the necessary transformations such as scaling, normalization and feature engineering.\n",
        "\n",
        "Load the preprocessed data into an SQLite database for storage and easy retrieval."
      ],
      "metadata": {
        "id": "h9nAqUybfNBK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To develop a Data ETL pipeline using Python, the first step is to collect data from a data source. Let’s use the Fashion-MNIST dataset provided by the Keras library "
      ],
      "metadata": {
        "id": "55heWFejfe_B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oj8hZ06FfGvc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f00696e7-543d-4805-b6ba-528abef00c61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "import tensorflow.keras as keras\n",
        "(xtrain, ytrain), (xtest, ytest) = keras.datasets.fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(xtrain.shape)\n",
        "print(ytrain.shape)\n",
        "print(xtest.shape)\n",
        "print(ytest.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUPGw6mVgVLR",
        "outputId": "134fb97e-cc65-44e5-8f3d-fff656153a87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28)\n",
            "(60000,)\n",
            "(10000, 28, 28)\n",
            "(10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. TRANSFORM**"
      ],
      "metadata": {
        "id": "dq0q2MskggvO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let’s clean and transform the data. Here we will normalize the pixel values to be between 0 and 1 and reshape the data into a 4D tensor"
      ],
      "metadata": {
        "id": "oViIBn17gcok"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WHY?**\n",
        "\n",
        "We need to normalize the pixel values to be between 0 and 1 in order to ensure that all the input features (i.e., the pixel values) have the same scale. This is important because machine learning algorithms often work better when the input features are on the same scale, as this helps to prevent some features from dominating others. For example, if one pixel has a range of 0-255 and another pixel has a range of 0-1, the first pixel may end up dominating the second pixel in the calculations. By normalizing the pixel values to be between 0 and 1, we ensure that all the input features are on the same scale and that no one feature dominates the others.\n",
        "\n",
        "Reshaping the data into a 4D tensor is also important because it allows us to represent the images as matrices or arrays with dimensions (samples, height, width, channels). The first dimension represents the number of images in our dataset, the second and third dimensions represent the height and width of each image, respectively, and the fourth dimension represents the number of channels (e.g., 1 for grayscale images and 3 for RGB images). By representing the images in this way, we can use convolutional neural networks (CNNs) to extract features from the images and classify them accurately."
      ],
      "metadata": {
        "id": "B7-5F1WEhBPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "xtrain = xtrain.astype('float32') / 255\n",
        "xtest = xtest.astype('float32') / 255\n",
        "\n",
        "xtrain = np.reshape(xtrain, (xtrain.shape[0], 28, 28, 1))\n",
        "xtest = np.reshape(xtest, (xtest.shape[0], 28, 28, 1))\n",
        "\n",
        "print(xtrain.shape)\n",
        "print(ytrain.shape)\n",
        "print(xtest.shape)\n",
        "print(ytest.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lH-t6I9LgaEZ",
        "outputId": "7f1ab421-41e8-4626-de5a-3fce14eb32b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28, 1)\n",
            "(60000,)\n",
            "(10000, 28, 28, 1)\n",
            "(10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. LOAD**"
      ],
      "metadata": {
        "id": "ptA31sBEhKyL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let’s load the data into a database. We can use SQLite to create a database and load the data into it"
      ],
      "metadata": {
        "id": "OCdeVgIihJRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "\n",
        "conn = sqlite3.connect('fashion_mnist.db')\n",
        "\n",
        "conn.execute('''CREATE TABLE IF NOT EXISTS images\n",
        "             (id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "             image BLOB NOT NULL,\n",
        "             label INTEGER NOT NULL);''')\n",
        "\n",
        "for i in range(xtrain.shape[0]):\n",
        "    conn.execute('INSERT INTO images (image, label) VALUES (?, ?)',\n",
        "                [sqlite3.Binary(xtrain[i]), ytrain[i]])\n",
        "\n",
        "conn.commit()\n",
        "\n",
        "for i in range(xtest.shape[0]):\n",
        "    conn.execute('INSERT INTO images (image, label) VALUES (?, ?)',\n",
        "                [sqlite3.Binary(xtest[i]), ytest[i]])\n",
        "\n",
        "conn.commit()\n",
        "\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "uAThm0N4god8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code:\n",
        "\n",
        "1. The first line imports a library called sqlite3, which allows us to work with SQLite databases in Python.\n",
        "2. We then create a connection to the database.\n",
        "3. Next, we create a table in the database called “images”.\n",
        "4. We use a loop to loop through each image in the training data and insert it into the “images” table (along with the labels).\n",
        "5. We use the commit() method to save the changes we made to the database.\n",
        "6. We then use another loop to loop through each image in the test data and insert it into the “images” table (along with the labels).\n",
        "7. We use the commit() method again to save the changes we made to the database.\n",
        "8. Finally, we close the connection to the database."
      ],
      "metadata": {
        "id": "qE4vok_NhYzO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So this is how we can create a Data ETL pipeline using Python. Our ETL pipeline takes the Fashion MNIST dataset and stores it in an SQLite database so that we can easily access and manipulate the data later."
      ],
      "metadata": {
        "id": "-sdmvZ_khsYi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading the data you stored on the SQLite database"
      ],
      "metadata": {
        "id": "8nvWtRUchxsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "conn = sqlite3.connect('fashion_mnist.db')\n",
        "cursor = conn.cursor()\n",
        "\n",
        "cursor.execute('SELECT * FROM images')\n",
        "rows = cursor.fetchall()\n",
        "\n",
        "import pandas as pd\n",
        "data = pd.read_sql_query('SELECT * FROM images', conn)"
      ],
      "metadata": {
        "id": "xqgeUi8lhS4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data ETL is a process where data is extracted from a place, then the data is transformed in some way, and then data is loaded into a database. So ETL stands for Extracting, Transforming, and Loading the data. I hope you liked this article on developing a Data ETL pipeline using Python."
      ],
      "metadata": {
        "id": "V0iIAQPgh6_W"
      }
    }
  ]
}